Dubbo底层也是Netty

4.1.56



Maven依赖

```java
    <dependencies>
        <dependency>
            <groupId>io.netty</groupId>
            <artifactId>netty-all</artifactId> <!-- Use 'netty-all' for 4.0 or above -->
            <version>4.1.51.Final</version>
            <scope>compile</scope>
        </dependency>
    </dependencies>
```







Netty 架构及线程模型

1. 传统I/O模型

2. Reactor 模型

   * 单Reactor 单线程

   * 多Reactor 多线程
   * 主从Reactor 多线程

3. Netty线程模型





传统阻塞IO线程模型

1，针对每个连接，都需要一个线程

2，通过阻塞IO模型来进行数据的读写

问题：

1，当连接非常多的时候，需要特别多的线程。线程是有开销的。（每个线程都需要分配栈内存的。）

2，当一个连接上没有读写的时候，会阻塞在那里，比如read方法阻塞，此时会线程闲置，造成资源浪费。

3，线程上下文切换也要花时间





Reactor线程模型

1，基于IO多路复用。应用程序只需要在一个阻塞对象上等待，监听所有的客户端连接。

2，当有客户端来连接时候，建立连接，并把连接派发给一个线程池中的一个工作线程来处理。这样基于线程池复用线程资源， 不需要为每个连接创建线程。（一个处理线程是可以处理多个连接的业务的。）



也是基于事件的， 可以理解为  发布订阅模式



核心说明：

1，Reactor 在一个单独的线程中运行，负责监听和事件分发。分发给合适的处理程序（或者线程）对IO事件做出反应。

2，工作线程，专门负责处理IO事件，比如读写，然后进行业务处理。







单Reactor 单线程

单 reactoe 单线程，就是nio群聊的那种方式，reactor和handler在一个线程。需要验证一下的。监听，读和转发，都是在哪个线程执行的。
也就是监听的和业务处理的是属于同一个线程执行的。如果客户端多，都要read影响效率

select io多路复用，可以实现一个阻塞对象监听多个连接请求

reactor通过select监听客户端请求，收到事件后进行dispatch分发

如果是建立连接的请求，就交给acceptor来处理，创建一个连接对象与客户端通信

如果是读写事件，就交给对应的handler来处理

handler会完成读，业务处理和写出



单reactor多线程。
reactor通过select监听请求。收到！请求事件后，通过dispatch进行分发
如果是连接请求，就通过acceptor处理，然后创建一个handler处理后续的各种事件
如果读写事件，reactor会分发到连接对应的handler来处理
handler只进行读写，不负责业务。handler读取取数据后，分发给worker线程池
worker线程池会分配合适的线程完成业务，并把结果返回给handler，然后handler返send到客户端
优点，将业务处理拆分到线程池去执行，可以利用cpu多核缺点，reacor处理所有事件的监听和响应，并且在单线程，高并发场景会出现性能瓶颈

主从reactor
main  reactor  只负责连接
连接建立后，main将连接交给sub来处理
sub监听在这个链接上的读写事件，如果发生了读写事件，就交给连接对应的handler来进行io处理
handler只read  write ，业务处理还是交给线程池
io放在sub reactor
业务放在worker线程池

一个端口，可以有多个selector么？？
serversocketchannel是绑定端口，并注册到selector。监听连接。
连接的时候，服务器用一个端口，后续读写，是不是用了其他端口呢？？
连接建立之后，把channel注册到worker的selector上。监听读写。

对于一个客户端，至少需要两个selector。

都是通一个端口啊









IDEA  下载netty 及源码文档

projectstructure -》library -》 + -》 from Maven -》 io.netty:netty-all  

就会下载到lib目录下。


![](https://cdn.jsdelivr.net/gh/fanshanhong/note-image/Netty_download_source.png)


